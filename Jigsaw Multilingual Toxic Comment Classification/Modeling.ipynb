{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q googletrans\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "\n",
    "import transformers\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "from googletrans import Translator\n",
    "from colorama import Fore, Back, Style, init\n",
    "import plotly.graph_objects as go\n",
    "translator = Translator()\n",
    "\n",
    "from tensorflow.keras.layers import (Dense, Input, LSTM, Bidirectional, Activation, Conv1D, \n",
    "                                     GRU,Embedding, Flatten, Dropout, Add, concatenate, MaxPooling1D,\n",
    "                                     GlobalAveragePooling1D,  GlobalMaxPooling1D, \n",
    "                                     GlobalMaxPool1D,SpatialDropout1D)\n",
    "\n",
    "from tensorflow.keras import (initializers, regularizers, constraints, \n",
    "                              optimizers, layers, callbacks)\n",
    "\n",
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set1 = pd.read_csv(os.path.join( 'jigsaw-toxic-comment-train.csv'))\n",
    "train_set2 = pd.read_csv(os.path.join( 'jigsaw-unintended-bias-train.csv'))\n",
    "train_set2.toxic = train_set2.toxic.round().astype(int)\n",
    "\n",
    "valid = pd.read_csv(os.path.join('validation.csv'))\n",
    "test = pd.read_csv(os.path.join( 'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train1 with a subset of train2\n",
    "train = pd.concat([\n",
    "    train_set1[['comment_text', 'toxic']],\n",
    "    train_set2[['comment_text', 'toxic']].query('toxic==1'),\n",
    "    train_set2[['comment_text', 'toxic']].query('toxic==0').sample(n=100000, random_state=0)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### < fast encoder >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_encode(texts, tokenizer, chunk_size=240, maxlen=512):  # chunk_size 지정\n",
    "    tokenizer.enable_truncation(max_length=maxlen)\n",
    "    tokenizer.enable_padding(max_length=maxlen)\n",
    "    all_ids = []\n",
    "    \n",
    "    for i in range(0, len(texts), chunk_size):\n",
    "        text_chunk = texts[i:i+chunk_size].tolist()\n",
    "        encs = tokenizer.encode_batch(text_chunk)\n",
    "        all_ids.extend([enc.ids for enc in encs])\n",
    "    \n",
    "    return np.array(all_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### < general encoder >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regular_encode(texts, tokenizer, maxlen=512):\n",
    "    enc_di = tokenizer.batch_encode_plus(\n",
    "        texts, \n",
    "        return_attention_masks=False, \n",
    "        return_token_type_ids=False,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=maxlen\n",
    "    )\n",
    "    return np.array(enc_di['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TPU config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Please provide a TPU Name to connect to.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\keras\\api\\_v1\\keras\\layers\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Create strategy from tpu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtpu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster_resolver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTPUClusterResolver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_connect_to_cluster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtpu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtpu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize_tpu_system\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtpu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\distribute\\cluster_resolver\\tpu_cluster_resolver.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, tpu, zone, project, job_name, coordinator_name, coordinator_address, credentials, service, discovery_url)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtpu\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Please provide a TPU Name to connect to.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tpu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtpu\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# self._tpu is always bytes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Please provide a TPU Name to connect to."
     ]
    }
   ],
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# Create strategy from tpu\n",
    "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "tf.config.experimental_connect_to_cluster(tpu)\n",
    "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "\n",
    "# Configuration\n",
    "EPOCHS = 4\n",
    "BATCH_SIZE = 16* strategy.num_replicas_in_sync\n",
    "MODEL = 'jplu/tf-xlm-roberta-large'\n",
    "MAX_LEN = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 딥러닝; LSTM - RNN - NN(Neural Network) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Neural Network\n",
    "\n",
    "#### 인간의 신경체계를 모방한 정보처리 시스템 - 복잡, 비선형, 병렬계산\n",
    "- 대량의 병렬 분산된 처리소자 -> 처리 노드가 많기 때문에 몇몇 노드나 연결에서의 결함이 시스템 전체의 심각한 결함을 초래하지는 않음\n",
    "- 학습 알고리즘 -> 신경망의 출력값을 관찰하여 바람직한 출력값이 계산되도록 연결강도를 조정.  \n",
    "- 분류 작업 수행\n",
    "- 복잡한 비선형적 모델 추정하는데 유리 (비수렴성의 문제가 있긴 함)\n",
    "- decision tree나 회귀와 같은 모델과는 달리 가중치의 해석/ 입력변수의 중요성 판단하기 어려움\n",
    "\n",
    "\n",
    "\n",
    "### 분류 (구조)\n",
    "- 단층 전방향 신경망\n",
    "- 다층 전방향 신경망 --> RNN\n",
    "- 순환 신경망\n",
    "- 격자 신경망 (Self Organizing Map) : 비감시\n",
    "\n",
    "![title](1.jpg)\n",
    "\n",
    "#### 퍼셉트론 - 인공신경망의 시초로, 다수의 트레이닝 데이터를 기반으로 지도학습을 수행하는 알고리즘\n",
    "- Xn : 입력값(데이터 특성을 나타내는 값)\n",
    "- Wn : 가중치\n",
    "- 순입력함수 : 특성값에 가중치를 곱한 값을 만드는 함수\n",
    "- 활성함수 : 순입력함수의 결과값을 특정 임계값과 비교하여 이를 바탕으로 1 or -1 출력하는 함수\n",
    "\n",
    "#### 즉, 위의 그림을 요약하면,\n",
    "![title](2.jpg)\n",
    "\n",
    "- 위의 그림과 같은 뉴런이 대량으로 연결되어 있다고 보면됨\n",
    "- 입력층은 다른 노드의 출력값이 입력되는 것이고, 출력층은 어떤 노드의 출력값이 다른 노드로 전달되는 층이라고 보면됨\n",
    "- 위와 같이 중간층이 하나의 노드로 구성되어 중간층과 출력층의 구분이 없는 구조를 단층 퍼셉트론이라함(중간층이 다수로 구성되어 있는 구조는 다층 퍼셉트론)\n",
    "\n",
    "#### 다층 퍼셉트론 (MLP)\n",
    "![title](3.jpg)\n",
    "\n",
    "- 다층 퍼셉트론에서 입력층과 출력층 사이에 존재하는 층을 은닉층이라고 함\n",
    "- 은닉층이 2개 이상인 신경망을 심층 신경망(DNN)이라고 함\n",
    "- 이러한 다층 인공신경망을 학습하는 알고리즘을 딥러닝이라고 함\n",
    "\n",
    "\n",
    "#### 다층 퍼셉트론 신경망은 입력이 출력방향으로만 활성화되고, 은닉 뉴런이 과거의 정보를 기억하지 못한다는 단점이 있음 -> RNN이 해결\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - RNN(Recurrent Neural Network)\n",
    "#### 인공신경망 구조에서 순환 신경망으로 분류됨\n",
    "<img src=\"4.jpg\" width=\"400\">\n",
    "<img src=\"5.jpg\" width=\"400\">\n",
    "\n",
    "- 히든노드(초록색 박스)가 방향을 가진 엣지로 연결되어 순환구조를 이루는 인공 신경망 -> 시퀀스 데이터의 모델링 가능\n",
    "- '기억'을 갖고 있다는 특징이 있음(순전파) -> 지금까지의 입력데이터를 요약한 정보\n",
    "- 즉, 새로운 입력이 들어올때마다 네크워크가 자신의 기억(이전 layer의 weight)을 조금씩 수정함(역전파) -> 입력을 모두 처리하면 남겨진 기억은 시퀀스 전체를 요약하는 정보\n",
    "- Xt : 인풋, Ht : 히든(은닉) 노드, Yt : 출력값, Bn : 편향값\n",
    "- 히든노드의 활성함수는 비선형함수인 tanh(역삼각함수)\n",
    "\n",
    "\n",
    "\n",
    "### RNN이 할 수 있는 것?\n",
    "1. 고정크기 입력 & 시퀀스 출력 : 이미지를 입력하면, 이미지의 캡션 자동 생성\n",
    "2. 시퀀스 입력 & 고정크기 출력 : 문장을 입력해 긍/부정 정도를 출력하는 감정 분석기\n",
    "3. 시퀀스 입력 & 시퀀스 출력 : 영어를 한국어로 번역하는 자동 번역기\n",
    "4. 동기화된 시퀀스 입력 & 시퀀스 출력 : 문장에서 다음에 나올 단어 예측\n",
    "\n",
    "#### RNN은 관련된 정보와 그 정보를 출력하는 지점의 사이가 멀어질수록 역전파시 gradient가 줄어들어 학습능력이 저하되는 단점(vanishing gradient problem)을 가지고 있음 --> LSTM이 해결"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - LSTM (Long Short Term Memory)\n",
    "#### RNN의 hidden-state에 cell-state를 추가\n",
    "<img src=\"6.jpg\" width=\"500\">\n",
    "\n",
    "\n",
    "- cell-state는 4개의 특별한 상호작용을 함\n",
    "- C는 기억/망각 정도를 학습한 셀\n",
    "- forget gate는 이전의 state를 얼마나 잊을지 결정\n",
    "- input gate는 현재 input으로 들어온 정보를 얼마나 기억할지 결정\n",
    "- output gate는 얼마나 밖으로 표출할지 결정\n",
    "\n",
    "<img src=\"7.jpg\" width=\"400\">\n",
    "위의 gate들은 hidden unit과 x를 받아 활성함수로 sigmoid함수를 거쳐 0과 1사이 값을 출력함"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
